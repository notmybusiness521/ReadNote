文章地址：https://zhuanlan.zhihu.com/p/655402388
这篇文章讨论了为什么在训练大型AI模型时，通常选择使用NVIDIA的A100或H100 GPU而不是新推出的4090显卡。文章首先指出，在性价比上，用于推理任务的4090显卡可能比H100显卡更有优势，但在大模型训练方面，4090显卡存在一些限制。
主要观点和分析包括：
GPU性能对比：文章列出了H100、A100和4090在Tensor FP16算力、Tensor FP32算力、内存容量、内存带宽、通信带宽和通信时延等关键性能指标上的对比。
成本分析：对H100和4090的成本进行了分析，认为H100的售价存在较高的利润空间。
大模型训练需求：讨论了大模型训练对算力、内存带宽、内存容量和通信数据量的需求，以及如何计算所需的GPU数量。
并行方式：介绍了数据并行、流水线并行和张量并行三种并行方式，并分析了它们在大模型训练中的应用和限制。
4090在训练中的局限性：指出4090显卡在内存容量、通信效率和许可证约束方面的不足，导致其不适合用于大模型训练。
推理任务的优势：分析了4090在推理任务中的优势，包括性价比和内存带宽对推理性能的影响。
KV Cache：解释了KV Cache在推理中的作用，以及它如何减少计算量和内存带宽需求。
推理性能优化：探讨了推理性能优化的挑战，包括batch size的选择和网络延迟对流水线并行推理的影响。
成本效益分析：对使用4090和H100进行推理的成本效益进行了比较，包括硬件成本、能耗和吞吐量。
未来展望：文章最后提出了一些创新的想法，例如使用区块链技术作为大模型推理的证明机制，以及家庭局域网络在AI推理中的潜力。
文章的结论是，尽管4090在某些方面表现出色，但在大模型训练方面，A100和H100由于其更高的内存带宽、更大的内存容量和更优的通信性能，仍然是更合适的选择。而在推理任务中，4090可以提供具有竞争力的性能和成本效益。
